%%	SECCION documentclass																									 %%	
%%---------------------------------------------------------------------------%%
\documentclass[a4paper]{report}
%%---------------------------------------------------------------------------%%
%%	SECCION usepackage																											 %%	
%%---------------------------------------------------------------------------%%
\usepackage{amsmath, amsthm}
\usepackage{amsfonts}%
\usepackage{amssymb}%
\usepackage{caratula}
\usepackage{hyperref} %para que haya hiperv�nculos
\usepackage{graphicx} % Para el logo magico!
\usepackage{alltt} % que es este paquete???

\usepackage[spanish]{babel} 
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage[utf8]{inputenc}
%\usepackage[latin1]{inputenc}

\oddsidemargin 0cm
\headsep -1.4cm
\textwidth=16.5cm
\textheight=23cm
\setlength{\parskip}{0.3cm}

%\makeindex



%%---------------------------------------------------------------------------%%
%%	SECCION document	                                                       %%	
%%---------------------------------------------------------------------------%%
\begin{document}
	\setcounter{page}{0} %para que numere a la caratula como p�gina 0 y a las demas a partir de 1

%%---- Caratula -------------------------------------------------------------%%
\title{Trabajo Práctico}
\author{Matias, Rodrigo y Martin\thanks{This is for making an acknowledgement.}
\\Universidad de Buenos Aires, Argentina}
\date{14 April, 2009}

\materia{Teoria de lenguajes}
\titulo{Trabajo Práctico}
\subtitulo{Generador de analizadores sintácticos}
\fecha{18 de junio, 2009}

\integrante{Rodrigo Campos}{561/06}{rodrigo@sdfg.com.ar}
\integrante{Martín Fernandez}{539/06}{bondi007@gmail.com}
\integrante{Matías Pérez}{002/05}{elmaildematiaz@gmail.com}
%\resumen{}

\maketitle

%\tableofcontents

\newpage

\clearpage

\section*{Consideraciones generales}

El trabajo práctico fué realizado en \emph{Python2.6}. Y el mismo genera codigo en \emph{C++}.

\section*{Análisis del lenguaje de especificación de gramáticas}

Tal como planteaba el enunciado de este trabajo práctico, el conjunto de producciones pertenecientes a la gramática que definirá el lenguaje para el cual finalmente se generará el analizador sintáctico pertinente, se encontraba expresado en un lenguaje que especifica producciones de gramática. Para dicho lenguaje se conocía, mediante el enunciado, su conjunto de producciones. Así, por ejemplo la cadena \emph{ A:aB | C ; } escrita en el lenguaje mencionado, representaba la producción \emph{A -> aB | C}, para cualquier gramática que la incluyera entre sus producciones.


Dada esta situación, se tornaba fundamental comprender qué producción estaba representando efectivamente cada cadena escrita en el mencionado lenguaje de especificación. Además de esto, era indispensable poder verificar si dada una cadena escrita con los símbolos que define el lenguaje de especificación en cuestión, la misma pertenece o no al lenguaje.


Ante estas necesidades, decidimos utilizar un analizador sintáctico llamado \emph{pyparsing}, para el lenguaje de programación \emph{python}. Este analizador nos ofrecía las funcionalidades requeridas y por ende nos permitía verificar si dada una cadena pertenecía al lenguaje de gramáticas que debemos aceptar la mismo tiempo que nos permite construir el grafo correspondiente a la misma.

%%Sin embargo, en el momento de intentar utilizar pyparsing para analizar el lenguaje de especificaciones nos encontramos con que el conjunto de producciones dado en el enunciado que definía la gramática de dicho lenguaje, conformaba una gramática ambigüa. Por ende, antes de proseguir con el desarrollo, nos encontramos ante la necesidad de desambigüar la gramática definida en el enunciado de este trabajo práctico. Con lo cual pasamos de tener inicialmente el siguiente conjunto de producciones:

%%\begin{itemize}
%%	\item G  -> P G | P
%%	\item P  -> m ':'  PD ';'
%%	\item PD -> n | t | $\lambda$ |PD | PD PD | PD '*' | PD '+' | PD '?'  | '(' PD ')'
%%\end{itemize}

%%(en donde \emph{t} representa a un caracter en minúscula, que a su vez representa un símbolo terminal en la gramática que se intenta definir. El comportamiento de \emph{n} es análogo, pero respecto a los no terminales de la gramática a definir, para lo cual n es un caracter en mayúscula).

%%a tener finalmente el conjunto de producciones desambigüado que se presenta a continuación

%%\begin{itemize}
%%	\item G 		-> Prod (OtrasProd)*
%%	\item Prod 	-> n ':' ProdDer ';'
	
%%\end{itemize}

%%Una vez desambigüada la gramática, podiamos utilizar finalmente pyparsing para analizarla. Ahora, nos resultaba bastante sencillo determinar si una cadena representaba o no una producción válida construida en base a la gramática del lenguaje de especificacion de gramáticas. Sin embargo, todavía nos quedaba la tarea de generar el grafo con el cual trabajarían los algoritmos de cálculo de \emph{anulables}, \emph{primeros}, \emph{siguientes}, y \emph{símbolos directrices}, que son explicados posteriormente. En un principio creimos que podríamos realizar ambas tareas ("`parsear"' la gramática inicial y armar el grafo correspondiente a la gramática definida por la gramática inicial) en forma secuencial. Es decir, primero utilizar pyparsing para verificar que cada producci\'n definida estuviera bien construida y perteneciera al lenguaje y luego en base a un algoritmo implementado por nosotros construir el grafo en cuestión. Lamentablemente, el intento resultó fallido, dado que la implementación de dicho algoritmo debía contemplar una gran cantidad de casos particulares, y resultaba ademas de "`endeble"', complicado de explicar y de entender y extremadamente complejo a la hora de realizar tares de test y posterior "`debug"'. Con lo cual, decidimos encarar la solución de este problema por otro camino. Tomamos la decisión entonces de revisar la documentación de pyparsing con el objetivo de verificar si el mismo no nos podía "`dar una mano"' con la construcción del grafo. Finalmente, descubrimos que pyparsing permitía realizar llamados a funciones auxiliares durante el proceso de análisis de una cadena. De esta manera, cada vez que encontrabamos un operador (como ser '*', '+', '?'), un conector (como ser '|', o implicitamente un conector de concatenación), un símbolo no terminal, o uno terminal podíamos llamar a la función auxiliar correspondiente a cada uno de ellos que se encargaba de, en cada caso, modificar o seguir construyendo (o ambas simultaneamente) el grafo representativo de la gramática en cuestión.

%%No me parece que esto fuera importante en el informe, no creo que les interese saber qué cosas hicimos que no nos salieron

\subsection*{Breve reseña acerca de pyparsing}

Pyparsing es un analizador sintáctico que, como tal, permite enunciar un conjunto de reglas sobre las cuales se construyen las cadenas válidas de un lenguaje. Estas reglas guardan similitud con las producciones que usualmente se utilizan para describir el comportamiento de una gramática. 

Mientras que en el lenguaje natural de gramáticas el símbolo \emph{->} se utiliza para denotar la producción derecha que genera un símbolo no terminal, en pyparsing este símbolo se representa con \emph{=}, con lo cual una producción de la forma \emph{A -> aB}, en pyparsing se reescribiría como \emph{A = a + B} (en donde el símbolo '+' representa la concatenación en el lenguaje usual de producciones). 

Por otra parte, el operador '|' se representa en pyparsing con el mismo símbolo. Además, el valor $\lambda$ es representado con la sentencia \emph{Empty()}. La funcionalidad del símbolo '*', por otra parte, es representada por la función \emph{ZeroOrMore( cadena )}, mientras que las acciones del símbolo '+' son llevadas a cabo por la funcón \emph{OneOrMore()}. 

Por ser python un lenguaje de programación, no podemos enunciar la producción \emph{A = a + B} sin haber definido antes que es 'B'. Esto puede ocasionar un problema si el símbolo inicial de la gramática tiene en su producción un no terminal todavía no definido. Para solucionar esto, alcanza con definir que \emph{B} es una ''producción´´ a definir luego, esto se hace con la cláusula \emph{Forward}. Y luego se define a \emph{B} como ya se detalló pero usando $<<$ en vez de \emph{=}.

Otras funciones que resultaron de utilidad a la hora de utilizar pyparsing son:

\begin{itemize}
	\item setParseAction( nombreDeFuncion ), que fue utilizada en el momento de la creación del grafo correspondiente a la gramática que se parsea. Así, por ejemplo, OneOrMore(valor).setParseAction(esConcat) invocará a la función esConcat cuando ocurrar que la cadena valor aparezca una o mas veces consecutivamente .
	\item Literal( caracter ), esta función sirve para definir caracteres a matchear (Es \emph{case sensitive}). 
\end{itemize}

\subsection*{Especificación del Lenguaje de gramáticas}

El lenguaje de gramáticas dado en el enunciado fué:

\begin{itemize}
\item G $->$ P G | P
\item P $->$ n ':' PD ';'
\item PD $->$ n | t | $\lambda$ | PD '|' PD | PD PD | PD '*' | PD '+' | PD '?' | '(' PD ')'
\end{itemize}

Donde 'n' son letras mayúsculas y 't' son minúsculas.



Para explicarle a \emph{pyparsing} cómo se compone nuestro lenguaje, primero empezamos por definirle las átomos del mismo. Es decir, empezamos por definirle las cosas más pequeñas que son un elemento a detectar por si mismo. Estos son:
\begin{itemize}
\item Los caracteres en minuscula: Estos son nuestros nodos Terminales.
\item Los caracteres en mayuscula: Estos son nuestros nodos No-Terminales.
\item Los simbolos '?', '*', '+'
\item La barra invertida '\textbackslash' que representa $\lambda$
\item Y por último diferenciamos al simbolo distinguido del resto de los No-Terminales por cuestiones prácticas.
\end{itemize}



Una vez definidos los átomos de nuestro lenguaje pasamos a definirle la sintáxis:
\begin{itemize}
\item[1] valorMin $->$ noterminal | '(' produccionDer ')' | terminal | $\lambda$
\item[2] valor $->$ valorMin? signo+ | valorMin
\item[3] concat $->$ valor+
\item[4] produccionDer $->$ '|'* concat? ('|' concat?)*
\item[5] prod $->$ noterminal ':' produccionDer ';'
\item[6] produccionInicial $->$ simbDisting ':' produccionDer ';'
\item[7] gramaticaInicio $->$ produccionInicial prod*
\end{itemize}

Los items 5 y 6 son casi el mismo, los diferenciamos para poder diferenciar facilmente la produccion inicial del resto de las producciones. Por otro lado el ''simbolo distinguido´´ sería \emph{gramaticaInicio}, ya que por esa se empieza a parsear. Se puede ver entonces que si produccionDer se comporta como PD en la gramática del enunciado se genera el mismo lenguaje. Por otro lado también se puede observar que de no haber conflictos en el parseo de produccionDer tampoco va a haber problemas en el parseo de la gramática, ya que el parseo de los items 5 a 7 es trivial.

Por otro lado los items 1 y 3 si se asume que valor y produccionDer no tienen conflictos en el parseo es facil ver que estos tampoco los tendrán.

Entonces los que pueden llegar a tener conflictos son los items 2 y 4:
\begin{itemize}
\item Por un lado el item 2 (valor) puede tener problemas porque tiene recursion a izquierda, pero esto para pyparsing no genera conflictos porque el '|' de pyparsing tiene prioridad a izquierda. Es decir, primero tratará de ''matchear´´ con la primer opción (valorMin? signo+) y luego con la segunda (valorMin).

También vale aclarar que el '+' seguirá reconociendo los signos hasta que no haya más. En caso contrario se podrían generar conflictos de parseo, por ejemplo, al parsear a+*. Pero esta cadena se reconocería de la siguiente manera:
\subitem Primero se reconoce la 'a' como un valorMin
\subitem luego el signo '+'
\subitem y luego el signo '*'.

Por otro lado una concatenacion de letras del estilo 'ab' se reconoce ambas veces con la segunda opcion de la produccón 2, ya que no hay signos.
\item Por otro lado el item 4 también genera varias derivaciones posibles para una misma cadena. Pero esto no genera problemas de parseo, ya que el simbolo '*' también se comporta como el '+' (reconociendo siempre que pueda). Esto hace que primero reconozca \emph{todos} los '|' que haya antes de tratar de reconocer las concatenaciones (o concat). Luego si no hay nada más terimna y sino trata de reconocer una concatenación, y luego no hay más posibles problemas. Ya que lo que queda es ('|' concat?)* que no trae inconvenientes.
\end{itemize}

Entonces se puede concluir que problemas de parseo con pyparsing no va a haber (a no ser que se ingrese una cadena inválida).\footnote{Para más información a cerca de pyparsing consultar http://pyparsing.wikispaces.com/HowToUsePyparsing, http://onlamp.com/pub/a/python/2006/01/26/pyparsing.html y http://pyparsing.wikispaces.com }.

Lo único que queda por ver es que ambas gramáticas definan el mismo lenguaje. Esto es más complejo y no conocemos un metodo formal para probarlo. Pero se puede ver, como ya dijimos antes, que si produccionDer genera lo mismo que PD ambas generan lo mismo.

produccionDer intenta dividir en dos casos las producciones de PD; cuando es una lista de disyunciones o cuando es una lista de concatenaciones. Si es una lista de disyunciones se puede ver que cumple, y tambien cumple si es una lista de conjunciones ya que '|'* y ('|' concat?)* son ambos anulables, quedando \emph{concat} que intenta representar justamente eso, una lista de conjunciones.

Entonces \emph{concat} debe representar todas las otras opciones de PD que no sea PD'|'PD. Es decir: n | t | $\lambda$ | PD PD | PD '*' | PD '+' | PD '?' | '(' PD ')'.

Por otro lado \emph{valor} se encargará de representar PD '*' | PD '+' | PD '?'. Mientras que valorMin se encargará de representar n | t | '(' PD ')' | $\lambda$.

Para comprender las reglas que se le definen a pyparsing quizás es útil pensar en la siguiente gramática, que si bien no es igual a las reglas definidas, las mismas se basan en esta:
\begin{itemize}
\item G $->$ P G | P
\item P $->$ n ':' PD ';'
\item PD $->$  PC '|' PD | PC
\item PC $->$ V PC | V
\item V $->$ VM S | VM
\item VM $->$ '(' PD ')' | n | t | $\lambda$
\item S $->$ '*' | '+' | '?'
\end{itemize}


%valorMin = noterminal | Suppress('(') + produccionDer + Suppress(')') \
%			| terminal | lamb
%
%	# Los valores son los valores minimos con algún simbolo o lambda
%	valor = ((valorMin | Empty().setParseAction(makeLamb)) + OneOrMore(signo)).setParseAction(opUnario) \
%			| valorMin
%
%
%	# El '.' no está en la gramática que pasan
%	concat = OneOrMore(valor).setParseAction(esConcat)
%
%	produccionDer << (ZeroOrMore(Suppress('|').setParseAction(makeLamb)) + \
%			(concat | Empty().setParseAction(makeLamb)) + \
%			ZeroOrMore(Suppress('|') + (concat | \
%				Empty().setParseAction(makeLamb))) \
%			).setParseAction(esDisjunc)
%
%	prod = (noterminal + Suppress(':') + produccionDer + \
%			Suppress(';')).setParseAction(produccion)
%
%	produccionInicial = (simbDisting + Suppress(':') + produccionDer + \
%			Suppress(';') ).setParseAction(prodInic)
%
%	# Separo la inicial porque me va a ser más fácil
%	gramaticaInicio = produccionInicial + ZeroOrMore(prod)


\section*{Comprobar que la gramática sea ELL(1)}

Para asgurarnos que la gramática sea ELL(1), hacemos dos checkeos en dos
momentos distintos.


El primer chequeo que realizamos es ver si la gramática tiene simbolos inútiles (es
decir, verificamos que todos los no-terminales sean alcanzables y activos). En caso contrario
la gramática se rechaza informando el motivo. Luego, por el teorema visto en clase que dice:
\begin{itemize}
\item Para toda gramatica G reducida (osea, una en las cuales todos lo
	no-terminales son alcanzables y activos), si G es independiente de contexto
	y LL(1), entonces G no es recursiva a izquierda.
\end{itemize}


Checkeamos si tiene recursión a izquierda, y si es así, concluimos que no es
ELL(1).


Es decir, suponemos que la gramática es ELL(1). Sabemos entonces que no
tiene recursión a izquierda. Si al checkear vemos que tiene recursión a
izquierda, entonces llegamos a un absurdo que vino de suponer que era ELL(1). Es
decir que podemos concluir que la gramática no es ELL(1).


Para poder aplicar el teorema anterior es necesario ver que la gramática es
independiente de contexto. Para ver esto, podemos ver cómo se reescribiría cada
símbolo que agregan las expresions regulares en producciones de una gramática
independiente de contexto y ver entonces que cada gramática cuyas producciones
son expresiones regulares se puede transformar en una gramática independiente de
contexto que genera el mismo lenguaje.

\begin{itemize}
\item Por cada expresion A -> B | C, se puede transformar en dos producciones: A -> B
y A -> C.
\item Por cada expresion A -> B*, se puede transformar en: A -> BA y A -> $\lambda$
\item Por cada expresion A -> B+, se puede transformar en: A -> BA' y A' -> B y A' -> $\lambda$
\item Por cada expresion A -> B?, se puede transformar en: A -> B y A -> $\lambda$
\end{itemize}


Es fácil ver que ambas expresiones tienen el mismo lenguaje.


Para transformar una expresión regular cualquiera, lo que se hace es por cada
'B*' que tenga, se puede introducir un nuevo no-terminal A, tal que A -> BA y A
-> $\lambda$, y reemplazar 'B*' en la expresión regular por A. Haciendo lo mismo para el
resto de los símbolos siguiendo las transformaciones explicadas arriba, se puede
ver que cualquier expresión regular se puede transformar en una gramática
independiente de contexto equivalente.


Se puede notar también, de la forma de transformar las expresiones regulares,
que si la expresión regular tenía recursión a izquierda, entonces luego de
transformarlas también tendrán.


Es decir, si A -> B* tiene recursion a izquierda, entonces es A -> A*, y las
producciones generadas seran A -> AA y A -> $\lambda$, es decir que hay recursión a
izquierda. Es fácil ver que para el resto de los casos sucede lo mismo


El segundo checkeo para ver que sea ELL(1), es el que vimos en clase, que lo
hacemos luego de calcular anulables, primeros, siguientes y los simbolos
directrices. Es decir, checkeamos que para cada '|', los simbolos directrices de
sus hijos sean disjuntos y para cada '+', '*', '?', checkeamos que los primeros
del nodo hijo sea disjunto con los siguientes del nodo y que ningun hijo sea
anulable.


\section*{Chequeo de símbolos Inútiles}

	Para chequear que la gramática no tenga símbolos inútiles primero chequeamos que todos 
los simbolos sean alcanzables y luego que sean activos.


	Para chequear que los simbolos sean inalcanzables simplemente se recorre la componente 
conexa del nodo del simbolo distinguido marcando los nodos. Y una vez que se terminó de recorrer 
simplemente se verifica que todos los nodos se hayan marcado, de no ser así se busca un no-terminal
y se notifica que éste no cumple produciendo una excepción.


	Para chequear que todos los simbolos sean activos, recorremos el grafo partiendo desde el
simbolo distinguido. Y por cada nodo, si es un terminal, $\lambda$, '*' o '?', lo
marcamos como activo (pues siempre producen algo, ese terminal o $\lambda$). A cada
no-terminal o '|', lo marcamos como activo si alguno de sus hijos es activo. Y al
'.' y '+' los marcamos como activo sólo si todos sus hijos son activos. Esto lo
hacemos hasta que recorramos el grafo entero sin marcar ningún nodo nuevo como activo.


	Una vez que conocemos a todos los simbolos activos lo único que queda por ver es si algún 
simbolo de la gramática no lo és, y en caso de serlo se produce una excepción notificando el no terminal inactivo.


%	De esta forma tenemos cuáles son los nodos activos del grafo. Luego, lo
%que hacemos es sacar todos los links que tenga un nodo a un nodo inactivo,
%partiendo desde el símbolo distinguido. Es fácil ver que esto es correcto, pues:
%
%
%\begin{itemize}
%\item Si el nodo es no-terminal o '|', entonces esos casos, que nunca producirían nada,
%se pueden ignorar.
%
%%\item Si el nodo es un '*' o '+' y sus hijos no son activos,
%entonces lo reemplazamos por $\lambda$ y sacamos el link a su hijo, ya que es lo
%único que pueden producir 
%
%\item Si, en cambio, es un '.' (representación utilizada para la operación de concatenación) o un '+', el nodo está activo si todos sus hijos tienen la misma condición, por lo que si tienen un hijo inactivo ellos también lo son, y por tanto, el padre sacará el link hacia ellos. 
%\item Si no llegamos al no-terminal partiendo desde el simbolo distinguido, el no-terminal era inalcanzable, por lo que lo podemos ignorar.
%\end{itemize}
%
%
%	Luego de este procedimiento convertimos a los nodos inactivos en inalcanzables, 
%ya que los ``desligamos'' de la componente conexa que contiene al simbolo distinguido. Una vez 
%que tenemos esto lo único que hace falta es quitar los nodos inalcanzables, para esto simplemente 
%primero los reconocemos recorriendo la componente conexa y a medida que lo hacemos generamos 
%el conjunto de los nodos de la misma, para luego reemplazar al conjunto de nodos del grafo 
%por este nuevo conjunto.


	Finalmente, si pasa estos chequeos, el grafo se
corresponde al de una gramática reducida (pues todos sus símbolos no-terminales son
activos y alcanzables)


\section*{Checkear la recursión a izquierda}

	Para analizar la recursión a izquierda, usamos una idea muy similar a la que
utilizamos para realizar el cálculo de ``primeros''. Recorremos el grafo de la misma manera, las
diferencias son:


\begin{itemize}

\item Si el nodo actual es un terminal o $\lambda$, lo ignoramos.

\item Si es '|', '*', '?', '+' o un no-terminal, hacemos lo mismo que hace primeros y además, para cada hijo si es un no-terminal que no pertenece a rec\_iz del nodo actual, lo agregamos y macarmos que cambió algo en
esta iteración.

\item Si es '.', hacemos lo mismo que hace primeros para cada hijo, solo que además también si el hijo es un no-terminal lo agregamos a rec\_iz del
nodo actual.

\end{itemize}


	Como consecuencia de esto, al finalizar tendremos un diccionario, el cual dado un nodo nos
dice con qué no-terminales puede comenzar (por esto es que es muy similar a
primeros). Si algún nodo puede comenzar con él mismo, entonces tiene recursión a
izquierda. Si, por el contrario, ninguno de ellos puede comenzar con si mismo, entonces no se tiene
recursión a izquierda presente en la gramática.

	Si se encuentra recursion a izquierda se levanta una excepción explicando el porqué de la misma, es decir, donde se encontró la recursión. Vale recordar en este punto que como la gramática fué alterada para quitar producciones inútiles puede ser que la excepcion hable de un $\lambda$ que no existía en la gramática, pero que es consecuencia del procedimiento antes explicado.


\section*{Simbolos directrices}

Para calcular los simbolos directrices, primero calculamos para cada nodo, los
anulables, los primeros y los siguientes.


Luego creamos un diccionario que tenga una entrada por cada nodo del grafo cuyo
valor sea los primeros del mismo, y si dicho nodo es anulable, le agregamos los
siguientes del nodo.


Para todos estos algoritmos utilizamos la misma forma de recorrer el grafo:
recorremos el grafo hasta que en una recorrida entera del grafo no hayamos
agregado nada a lo que estamos calculando (anulables, primeros o siguientes). Si
no cambio nada en una recorrida completa del grafo, es porque entonces ya hemos
calculado lo que queríamos calcular.

\subsection*{Anulables}

	En cada paso (o nodo) entonces, lo que hacemos es fijarnos si el nodo
actual está en el conjunto de nodos anulables y sino y cumple ciertos
requisitos, lo agregamos y marcamos que cambió algo en esta iteración.


	Los requisitos dependen del nodo actual, si es un terminal, nunca es
anulable, por lo que no lo agregamos. Si es '*', '?' o '$\lambda$', siempre es anulable
por lo que lo agregamos. Si es un no-terminal o un '|', es anulable si alguno de
sus hijos es anulable. Y si es un '.' o '+', es anulable si todos sus hijos son
anulables.


	Al terminar de recorrer el grafo, tenemos el conjunto de nodos
anulables.

\subsection*{Primeros}

	En cada paso, lo que hacemos es fijarnos cierta relacion entre los primeros
del nodo actual y sus hijos, si no la cumplen los agregamos y marcamos que
cambió algo en esta iteración.


	Si el nodo actual es un no terminal y el caracter no pertenece a los
primeros del nodo actual, lo agregamos. Si es un $\lambda$, lo ignoramos. Si es un
'|', '*', '?', '+' o un no-terminal, entonces nos fijamos que contenga a los
primeros de todos sus hijos. Si no los contiene, los agregamos y marcamos que
algo cambio en esta iteracion. Y si es '.', nos fijamos si contiene a los
primeros de su primer hijo (el de más a la izquierda), si no los contiene lo
agregamos y marcamos que cambió algo. Si el primer hijo es anulable, nos fijamos
que contenga a los primeros del segundo hijo, si no los contiene los agregamos y
marcamos que cambió algo en esta iteración. Y así con los sucesivos hijos hasta
que no tenga más o encontrar uno no anulable.


	Cuando hayamos recorrido el grafo entero sin cambiar nada, entonces
sabemos que todas las condiciones se cumplen, por lo que es fácil ver que se ha
calculado los primeros de cada nodo correctamente.


\subsection*{Siguientes}

	En cada paso, lo que hacemos es fijarnos cierta relacion entre los
siguientes del nodo actual y sus hijos, si no la cumplen los agregamos y marcamos que
cambió algo en esta iteración.


	Si el nodo actual es un termian o $\lambda$, lo ignoramos. Si es un '|',
'*', '?', '+' o un no-terminal, entonces nos fijamos que los siguientes de todos
sus hijos incluyan a los siguientes del nodo actual. Si no es así, los incluímos
y marcamos que algo cambió en esta iteración. Si es un '.' nos fijamos que los
siguientes del '.' sean tambien siguientes de su último hijo (el de más a la
derecha), si no es así los agregamos y marcamos que cambió. Si el último hijo es
anulable, entonces nos fijamos que los siguientes de '.' también sean siguientes
del nodo anterior al último y lo agregamos si no los incluye. Y lo mismo con el
anterior y así sucesivamente hasta llegar a uno que no es anulable. También,
para cada dos hijos 'x' e 'y', consecutivos ('x' más a la izquierda que 'y'),
nos fijamos que los siguientes de 'x' contengan a los primeros de 'y', si no es
así los agregamos. Y si 'y' es anulable, nos fijamos que los siguientes de 'x'
contenga a los a los siguientes de 'y'.


	Cuando hayamos recorrido el grafo entero sin cambiar nada, entonces
sabemos que todas las condiciones se cumplen, por lo que es fácil ver que se ha
calculado los siguientes de cada nodo correctamente.


\section*{Generación de código}


Para generar el código que parsee las cadenas del lenguaje de la gramática que se 
obtiene como entrada cumpliendo con los requisitos pedidos se decidió elegir el lenguaje C++. Para hacerlo se procede de la siguiente forma.


Por un lado se tienen dos archivos \emph{ParserEll1.cpp} y \emph{Utilitario.cpp} estos contienen un main general y estandar
que lo único que hace es leer la cadena de entrada, setear un par de variables y llamar a la funcion ``parsear()'' 
que será generada en el archivo \emph{codigoParser.cpp} que es el que se generará. El archivo \emph{Utilitario.cpp} 
es el que contiene la lógica del match, que se encarga de ver si la letra pasada para realizar el match es la misma del \emph{TC} 
(Token Corriente), en caso de serlo aumenta el \emph{TC} y en caso contrario tira el error correspondiente.


Por otro lado se genera el código de parseo especifico para cada gramática que debe estar en \emph{``codigoParser.cpp''}.


El programa lo que hace es primero obtener los diccionarios \emph{siguientes} y \emph{primeros}, y una vez que se obtienen se imprimen una serie de lineas que corresponden a ciertos include genericos de las librerias que usará. Y luego se imprime para cada nodo No-Terminal una función, que tiene como nombre \emph{Porc\_<Nombre Nodo>()} y para escribir el cuerpo de la misma se recorre el subgrafo que define el nodo llegando hasta las hojas o los nodos no terminales que se encuentren, para cada hijo se escribe\footnote{Si el No-Terminal tiene más de una produción se lo toma como si fuera un ``|''.}:


\begin{itemize}
\item Si es un Terminal se hace un \emph{match(<NombreNodo>);}
\item Si es un No Terminal se hace un \emph{Porc\_<Nombre Nodo>()}
\item Si es un ``|'' se hace una serie de \emph{if} para cada uno de sus hijos preguntando si el \emph{TC} está en los simbolos directrices de ese nodo, y dentro de cada if se coloca el cuerpo de los hijos y se sigue. Y se termina con un \emph{else} para el caso de error en el que se encuentra un caracter inesperado.
\item Si es un ``.'' se escribe directamente el codigo de todos sus hijos en orden.
\item Si es un ``*'' se escribe un \emph{while} preguntando si en \emph{TC} se encuentra en los Primeros de ese nodo.
\item Si es un ``+'' se actua de manera análoga al ``*'' pero con un \emph{do while} en vez del \emph{while}.
\end{itemize}


De esta menera se escribe todo el codigo correspondiente a esta gramática.

\section*{Notas de utilización del trabajo}

Para generar el programa que analiza una gramática dada se debe:
\begin{itemize}
\item Primero se debe generar el código del programa, para lo cual hay que ejecutar: \emph{python parser.py $>$ ``codigoParser.cpp''} (recordar que se necesita \emph{python2.6} o compatible).
\item Luego hay que compilar el código, simplemente se debe compilar \emph{ParserEll1.cpp} con los archivos \emph{Utilitario.cpp} y \emph{codigoParser.cpp} en el mismo directorio. Esto se puede realizar ejecutando \emph{g++ ParserEll1.cpp -o parser}
\item Una vez obtenido el programa, este recibe la cadena a reconocer por consola. Por lo que se debe ejecutar \emph{./parser ``cadena''} para que reconozca la cadena \emph{cadena}.
\end{itemize}

\section*{Testing}

El trabajo fué testeado con 6 gramáticas, de las cuales tres no se aceptan por no ser ELL1 y en los otros casos se prueban para cada una un set de cadenas validas e inválidas.

Este set de test se encuentra en la carpeta gramaticas. Y para ejecutarlo hay un script llamado \emph{testear.sh}, con lo que ejecutando \emph{./testear.sh} se corren todos los casos de test.

\end{document}
